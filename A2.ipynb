{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a335af73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#***************************************************************************************\n",
    "#    Title: How To Perform Sentiment Analysis in Python 3 Using the Natural Language Toolkit (NLTK)\n",
    "#    Author: Shaumik Daityari and Haley Mills\n",
    "#    Date: 2019\n",
    "#    Availability: https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n",
    "#\n",
    "#   [Source code]. https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n",
    "#***************************************************************************************\n",
    "\n",
    "#***************************************************************************************\n",
    "#    Author:Justin O Barber\n",
    "#    Date: 2013\n",
    "#    [Source Code]. https://stackoverflow.com/a/20827919\n",
    "#***************************************************************************************\n",
    "\n",
    "#****************************************************************************************\n",
    "#   @incollection{SocherEtAl2013:RNTN,\n",
    "#   title = {{Parsing With Compositional Vector Grammars}},\n",
    "#   author = {Richard Socher and Alex Perelygin and Jean Wu and Jason Chuang and Christopher Manning and Andrew Ng and Christopher Potts},\n",
    "#   booktitle = {{EMNLP}},\n",
    "#   year = {2013}\n",
    "#\n",
    "#   [Dataset]. https://nlp.stanford.edu/sentiment/index.html\n",
    "#****************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca23c1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put any imports here\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import classify, NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b69e65d",
   "metadata": {},
   "source": [
    "#### Dataset:\n",
    "\n",
    "We used the movie reviews dataset from Stanford University. This dataset has 7 different files: original_rt_snippets.txt, dictionary.txt, sentiment_labels.txt, SOStr.txt, STree.txt, datasetSentences.txt and datasetSplit.txt. From these files, we used dictionary.txt and sentiment_labels.txt.\n",
    "\n",
    "The dictionary.txt file contains all of the processed phrases from the movie reviews with ids. For instance, \"It was a good movie\" | 1. This line contains the phrase \"It was a good movie\", and the id for it is 1. \n",
    "\n",
    "The sentiment_labels.txt file contains all of the ids and the sentiment values for the id's corresponding phrase. For instance, 1 | 0.5. This means that the phrase with id 1 has a sentiment value of 0.5. This dataset divided sentiment values into 5 subsets: [0, 0.2] very negative, (0.2, 0.4] negative, (0.4, 0.6] neutral, (0.6, 0.8] positive, (0.8, 1.0] very positive. To keep the project simple, we decided to change the subsets to [0, 0.5] negative and (0.5, 1.0] positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd24c974",
   "metadata": {},
   "source": [
    "#### Creating database:\n",
    "\n",
    "We first opened the two files and stored the contents into the corresponding variables, dictData and sentimentData. The first line in the sentimentData file was an irrelevant line to the data, hence we read the file starting from line 1, instead of line 0. \n",
    "\n",
    "The phrases in the dictionary file was not in the order of the id values, hence we first sorted the contents of this file according to the ids, to match the order of the sentimentData file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63ffbdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable to hold the path to the file\n",
    "filePath = \"./dataset/dictionary.txt\"\n",
    "\n",
    "# open the file as \"r\" or read only and store this opened file in f\n",
    "with open(filePath, \"r\", encoding = \"utf8\") as f:\n",
    "    # read the data from f and store it in the string variable \"data\"\n",
    "    dictData = f.readlines()[:]\n",
    "\n",
    "# create a variable to hold the path to the file\n",
    "filePath = \"./dataset/sentiment_labels.txt\"\n",
    "\n",
    "# open the file as \"r\" or read only and store this opened file in f\n",
    "with open(filePath, \"r\", encoding = \"utf8\") as f:\n",
    "    # read the data from f and store it in the string variable \"data\"\n",
    "    sentimentData = f.readlines()[1:]\n",
    "    \n",
    "# sort the data in the dictionary data in order of the matched id\n",
    "dictData.sort(key=lambda dictData : list(\n",
    "    map(int, re.findall(r\"\\|(\\d+)\", dictData)))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dac1287",
   "metadata": {},
   "source": [
    "We then created list variable, dataBase, to store the phrases and the labels (Positive or Negative), according to the sentiment score subsets stated above: [0, 0.5] negative and (0.5, 1.0] positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35e9e0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "dataBase = []\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# label data as positive or negative depending on the sentiment scores given\n",
    "# greater than 0.5 is a positive score, else, negative\n",
    "\n",
    "for data in dictData:\n",
    "    \n",
    "    # text variable stores phrase to store in dataBase list\n",
    "    text = (dictData[i].split('|')[0]).strip()\n",
    "    \n",
    "    # matke sure text is not in stop_words\n",
    "    if text not in stop_words:\n",
    "        if float((sentimentData[i].split('|')[1]).strip()) >= 0.5:\n",
    "            dataBase.append((text, \"Positive\"))\n",
    "        else:\n",
    "            dataBase.append((text, \"Negative\"))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1489008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function changes the list format of the data so that the first element \n",
    "# of each tuple is a dictionary of features; Creates a dictionary of vocabulary\n",
    "def get_all_words(data):\n",
    "    # create empty dictionary\n",
    "    return_set = {}\n",
    "    for passage in data:\n",
    "        \n",
    "        # passage[0] gets the phrase, because the list looks like: ('your hair', 'Positive')\n",
    "        for word in word_tokenize(passage[0]):\n",
    "            \n",
    "            # if the return_set is empty, initialize return_set\n",
    "            if not return_set:\n",
    "                return_set = {(word.lower())}\n",
    "                \n",
    "            # if return_set isn't empty, add to the dictionary\n",
    "            else:\n",
    "                return_set.add((word.lower()))\n",
    "                \n",
    "    return return_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c32ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From each phrase and sentiment value\n",
    "# we check if a feature or word in the dictionary of vocabulary we created above exist or not.\n",
    "\n",
    "# For each phrase and sentiment value, return_list stores all possible words in the vocabulary\n",
    "# and whether or not the words exist in the phrase, indicated by True or False\n",
    "def tokenize_words(all_words, data):\n",
    "    # create empty list\n",
    "    return_list = []\n",
    "    for d in data:\n",
    "        append_val = ({word: (word in word_tokenize(d[0])) for word in all_words}, d[1])\n",
    "        return_list.append(append_val)\n",
    "        \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7aa394",
   "metadata": {},
   "source": [
    "#### Training and testing the database:\n",
    "The range of train and test data is kept to a minimum, due to the lack of computer power.\n",
    "In theory, we should be able to train and test the data with the entire dataBase with a 70% (train) to 30% (test) ratio. \n",
    "\n",
    "We did not need to randomize the picking of train and test dataset, because the sentiment.txt was already in mixed order. In other words, the database doesn't list all positive phrases and then negative phrases. Because we labeled the phrases in the order of the id values listed in the sentiment.txt, the sentiment values are in mixed order or positives and negatives.\n",
    "\n",
    "Below is a snippet of the database, showing the mixed sentiment values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a551600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('!', 'Positive'), (\"'\", 'Positive'), (\"' (\", 'Negative'), (\"' ( the cockettes\", 'Positive'), (\"' ( the cockettes )\", 'Negative')]\n"
     ]
    }
   ],
   "source": [
    "print(dataBase[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cbf52b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.8666666666666667\n",
      "Most Informative Features\n",
      "                    this = True           Negati : Positi =     10.3 : 1.0\n",
      "            particularly = True           Negati : Positi =      9.1 : 1.0\n",
      "               nightmare = True           Negati : Positi =      7.9 : 1.0\n",
      "                     elm = True           Negati : Positi =      6.7 : 1.0\n",
      "                  street = True           Negati : Positi =      6.7 : 1.0\n",
      "                   clash = True           Negati : Positi =      5.5 : 1.0\n",
      "              artificial = True           Negati : Positi =      4.3 : 1.0\n",
      "                 between = True           Negati : Positi =      4.3 : 1.0\n",
      "                sardonic = True           Negati : Positi =      4.3 : 1.0\n",
      "                     see = True           Negati : Positi =      4.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    # Split data into train and test sets\n",
    "    train_data = dataBase[:300]\n",
    "    test_data = dataBase[300:315]\n",
    "    \n",
    "    # Tokenize training and test data\n",
    "    all_words_train = get_all_words(train_data)\n",
    "    train_features = tokenize_words(all_words_train, train_data)\n",
    "    \n",
    "    all_words_test = get_all_words(test_data)\n",
    "    test_features = tokenize_words(all_words_test, test_data)\n",
    "    \n",
    "    # Train Naive Bayes classifier\n",
    "    classifier = NaiveBayesClassifier.train(train_features)\n",
    "    print(\"Accuracy is:\", classify.accuracy(classifier, test_features))\n",
    "    classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4863f284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I didn't enjoy this at all. -> Negative\n"
     ]
    }
   ],
   "source": [
    "    # Test on custom input\n",
    "    custom_review = \"I didn't enjoy this at all.\"\n",
    "    custom_features = {word: (word in word_tokenize(custom_review.lower())) for word in all_words_train}\n",
    "    print(custom_review, \"->\", classifier.classify(custom_features))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
