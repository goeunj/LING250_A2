{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "a335af73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#***************************************************************************************\n",
    "#    Title: How To Perform Sentiment Analysis in Python 3 Using the Natural Language Toolkit (NLTK)\n",
    "#    Author: Shaumik Daityari and Haley Mills\n",
    "#    Date: 2019\n",
    "#    Availability: https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n",
    "#\n",
    "#   [Source code]. https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n",
    "#***************************************************************************************\n",
    "\n",
    "#***************************************************************************************\n",
    "#    Author:Justin O Barber\n",
    "#    Date: 2013\n",
    "#    [Source Code]. https://stackoverflow.com/a/20827919\n",
    "#***************************************************************************************\n",
    "\n",
    "#****************************************************************************************\n",
    "#   @incollection{SocherEtAl2013:RNTN,\n",
    "#   title = {{Parsing With Compositional Vector Grammars}},\n",
    "#   author = {Richard Socher and Alex Perelygin and Jean Wu and Jason Chuang and Christopher Manning and Andrew Ng and Christopher Potts},\n",
    "#   booktitle = {{EMNLP}},\n",
    "#   year = {2013}\n",
    "#\n",
    "#   [Dataset]. https://nlp.stanford.edu/sentiment/index.html\n",
    "#****************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "909db715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "ca23c1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put any imports here\n",
    "import re, nltk, math, string\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import classify, NaiveBayesClassifier\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "a30014e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function normalizes the database\n",
    "def remove_noise(data, stop_words = ()):\n",
    "    \n",
    "    # empty list to store normalized data\n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    # passage looks like this: ('data value', 'pos tag', 'sentiment value')\n",
    "    for passage in (data):\n",
    "        text = ''\n",
    "        # Within the if statement, if the tag starts with NN, the token is assigned as a noun. \n",
    "        # Similarly, if the tag starts with VB, the token is assigned as a verb\n",
    "        for token in word_tokenize(passage[0]):\n",
    "            if passage[1].startswith(\"NN\"):\n",
    "                pos = 'n'\n",
    "            elif passage[1].startswith('VB'):\n",
    "                pos = 'v'\n",
    "            else:\n",
    "                pos = 'a'\n",
    "                \n",
    "            # initialize the lemmatizer\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            # lemmatize the data value at index 0, hence passage[0] with it's tag\n",
    "            token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "            #store each token in text variable to maintain original phrase\n",
    "            if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "                text += token.lower() + ' '\n",
    "                \n",
    "        # store lemmatized phrase in cleaned_tokens variable along with sentiment value\n",
    "        if text:\n",
    "            cleaned_tokens.append((text.strip(), passage[-1]))\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "73ecb535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function changes the list format of the data so that the first element \n",
    "# of each tuple is a dictionary of features; Creates a dictionary of vocabulary\n",
    "def get_all_words(data):\n",
    "    \n",
    "    # create empty dictionary\n",
    "    return_set = {}\n",
    "    \n",
    "    for passage in (data):\n",
    "        \n",
    "        # passage[0] gets the phrase, because the list looks like: ('your hair', 'Positive')\n",
    "        for word in word_tokenize(passage[0]):\n",
    "            \n",
    "            # if the return_set is empty, initialize return_set\n",
    "            if not return_set:\n",
    "                return_set = {(word.lower())}\n",
    "                \n",
    "            # if return_set isn't empty, add to the dictionary\n",
    "            else:\n",
    "                return_set.add((word.lower()))       \n",
    "    return return_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "9bcb3937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From each phrase and sentiment value\n",
    "# we check if a feature or word in the dictionary of vocabulary we created above exist or not.\n",
    "\n",
    "# For each phrase and sentiment value, return_list stores all possible words in the vocabulary\n",
    "# and whether or not the words exist in the phrase, indicated by True or False\n",
    "def tokenize_words(all_words, data):\n",
    "    \n",
    "    # create empty list\n",
    "    return_list = []\n",
    "    \n",
    "    for d in data:\n",
    "        append_val = ({word: (word in word_tokenize(d[0])) for word in all_words}, d[1])\n",
    "        return_list.append(append_val)\n",
    "        \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b69e65d",
   "metadata": {},
   "source": [
    "#### Dataset:\n",
    "\n",
    "We used the movie reviews dataset from Stanford University. This dataset has 7 different files: original_rt_snippets.txt, dictionary.txt, sentiment_labels.txt, SOStr.txt, STree.txt, datasetSentences.txt and datasetSplit.txt. From these files, we used dictionary.txt and sentiment_labels.txt.\n",
    "\n",
    "The dictionary.txt file contains all of the processed phrases from the movie reviews with ids. For instance, \"It was a good movie\" | 1. This line contains the phrase \"It was a good movie\", and the id for it is 1. \n",
    "\n",
    "The sentiment_labels.txt file contains all of the ids and the sentiment values for the id's corresponding phrase. For instance, 1 | 0.5. This means that the phrase with id 1 has a sentiment value of 0.5. This dataset divided sentiment values into 5 subsets: [0, 0.2] very negative, (0.2, 0.4] negative, (0.4, 0.6] neutral, (0.6, 0.8] positive, (0.8, 1.0] very positive. To keep the project simple, we decided to change the subsets to [0, 0.5] negative and (0.5, 1.0] positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd24c974",
   "metadata": {},
   "source": [
    "#### Creating database:\n",
    "\n",
    "We first opened the two files and stored the contents into the corresponding variables, dictData and sentimentData. The first line in the sentimentData file was an irrelevant line to the data, hence we read the file starting from line 1, instead of line 0. \n",
    "\n",
    "The phrases in the dictionary file was not in the order of the id values, hence we first sorted the contents of this file according to the ids, to match the order of the sentimentData file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "63ffbdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable to hold the path to the file\n",
    "filePath = \"./dataset/dictionary.txt\"\n",
    "\n",
    "# open the file as \"r\" or read only and store this opened file in f\n",
    "with open(filePath, \"r\", encoding = \"utf8\") as f:\n",
    "    # read the data from f and store it in the string variable \"data\"\n",
    "    dictData = f.readlines()[:]\n",
    "\n",
    "# create a variable to hold the path to the file\n",
    "filePath = \"./dataset/sentiment_labels.txt\"\n",
    "\n",
    "# open the file as \"r\" or read only and store this opened file in f\n",
    "with open(filePath, \"r\", encoding = \"utf8\") as f:\n",
    "    # read the data from f and store it in the string variable \"data\"\n",
    "    sentimentData = f.readlines()[1:]\n",
    "    \n",
    "# sort the data in the dictbionary data in order of the matched id\n",
    "dictData.sort(key=lambda dictData : list(\n",
    "    map(int, re.findall(r\"\\|(\\d+)\", dictData)))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca443e6",
   "metadata": {},
   "source": [
    "#### Cleaning data:\n",
    "\n",
    "After putting dictData in the order of the id values, we split each phrase in dictData and only keep the data value.\n",
    "\n",
    "Ex. We take dictData[0], which looks like ('data value | id') and change it to look like ('data value')\n",
    "\n",
    "We then used the pos_tag function to get the tags for each phrase in dictData to determine the context for each phrase and store it in tagged_data. \n",
    "We then modified the tagged_data to store the data value, the pos tag and the sentiment value that corresponds to each phrase. The data tagged_data[0] now looks like this:\n",
    "\n",
    "    ('data value', 'pos tag', 'sentiment value')\n",
    "    \n",
    "We then removed the noise using the remove_noise function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "3be7b953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign values to dictData to only contain phrases without id\n",
    "i = 0\n",
    "while i < len(dictData):\n",
    "    \n",
    "    text = (dictData[i].split('|')[0]).strip()\n",
    "    dictData[i] = text\n",
    "    i += 1\n",
    "\n",
    "# use pos_tag function to get the tags for each phrase in dictData\n",
    "tagged_data = pos_tag(dictData)\n",
    "\n",
    "# assign values to tagged_data to include phrase, pos tag and corresponding sentiment value\n",
    "# no need to check and compare id values, because sentimentData and tagged_data are already in order of id\n",
    "j = 0\n",
    "while j < len(tagged_data):\n",
    "    text = (tagged_data[j][0], tagged_data[j][1], (sentimentData[j].split('|')[1]).strip())\n",
    "    tagged_data[j] =  text\n",
    "    j += 1\n",
    "\n",
    "# call remove_noise on tagged_data to remove stop words and normalize the data\n",
    "stop_words = stopwords.words('english')\n",
    "cleaned_data = remove_noise(tagged_data, stop_words = ())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dac1287",
   "metadata": {},
   "source": [
    "We then created list variable, dataBase, to store the phrases and the labels (Positive or Negative), according to the sentiment score subsets stated above: [0, 0.5] negative and (0.5, 1.0] positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "35e9e0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "dataBase = []\n",
    "\n",
    "# label data as positive or negative depending on the sentiment scores given\n",
    "# greater than 0.5 is a positive score, else, negative\n",
    "for data in cleaned_data:\n",
    "    if float((data[1]).strip()) >= 0.5:\n",
    "        dataBase.append((data[0], \"Positive\"))\n",
    "    else:\n",
    "        dataBase.append((data[0], \"Negative\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ba0f0",
   "metadata": {},
   "source": [
    "#### Training and testing the database:\n",
    "The range of train and test data is kept to a minimum, due to the lack of computer power.\n",
    "In theory, we should be able to train and test the data with the entire database with a 70% (train) to 30% (test) ratio. \n",
    "\n",
    "We did not need to randomize the picking of train and test dataset, because the sentiment.txt was already in mixed order. In other words, the database doesn't list all positive phrases and then negative phrases. Because we labeled the phrases in the order of the id values listed in the sentiment.txt, the sentiment values are in mixed order of positives and negatives.\n",
    "\n",
    "Below is a snippet of the database, showing the mixed order of sentiment values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "0a7abf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the cockettes', 'Positive'), ('the cockettes', 'Negative'), ('the cockettes provides a window into a subculture hell-bent on expressing itself in every way imaginable', 'Negative'), ('the cockettes provides a window into a subculture hell-bent on expressing itself in every way imaginable', 'Negative'), ('the cockettes provides a window into a subculture hell-bent on expressing itself in every way imaginable', 'Positive'), ('a nightmare on elm street', 'Negative'), ('a nightmare on elm street', 'Negative'), ('a nightmare on elm street or', 'Negative'), ('a nightmare on elm street or', 'Positive'), ('a nightmare on elm street or the hill', 'Negative')]\n"
     ]
    }
   ],
   "source": [
    "print(dataBase[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "cbf52b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.8\n",
      "Most Informative Features\n",
      "                    this = True           Negati : Positi =     10.5 : 1.0\n",
      "            particularly = True           Negati : Positi =      9.2 : 1.0\n",
      "               nightmare = True           Negati : Positi =      8.0 : 1.0\n",
      "                     elm = True           Negati : Positi =      6.8 : 1.0\n",
      "                  street = True           Negati : Positi =      6.8 : 1.0\n",
      "                   clash = True           Negati : Positi =      5.5 : 1.0\n",
      "              artificial = True           Negati : Positi =      4.3 : 1.0\n",
      "                 between = True           Negati : Positi =      4.3 : 1.0\n",
      "                sardonic = True           Negati : Positi =      4.3 : 1.0\n",
      "                     see = True           Negati : Positi =      4.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    # Split data into train and test sets\n",
    "    train_data = dataBase[:300]\n",
    "    test_data = dataBase[300:315]\n",
    "    \n",
    "    # Tokenize training and test data\n",
    "    all_words_train = get_all_words(train_data)\n",
    "    train_features = tokenize_words(all_words_train, train_data)\n",
    "    \n",
    "    all_words_test = get_all_words(test_data)\n",
    "    test_features = tokenize_words(all_words_test, test_data)\n",
    "    \n",
    "    # Train Naive Bayes classifier\n",
    "    classifier = NaiveBayesClassifier.train(train_features)\n",
    "    print(\"Accuracy is:\", classify.accuracy(classifier, test_features))\n",
    "    classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "4863f284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I didn't enjoy this at all. -> Negative\n"
     ]
    }
   ],
   "source": [
    "    # Test on custom input\n",
    "    custom_review = \"I didn't enjoy this at all.\"\n",
    "    custom_features = {word: (word in word_tokenize(custom_review.lower())) for word in all_words_train}\n",
    "    print(custom_review, \"->\", classifier.classify(custom_features))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
